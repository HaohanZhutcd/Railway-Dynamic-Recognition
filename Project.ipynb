{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMcjBEG3atc8SJfxRDcRm7v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"premium","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"r8XxIBN2Y3ZD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680682234160,"user_tz":-60,"elapsed":18504,"user":{"displayName":"Haohan Zhu","userId":"13394524445423274234"}},"outputId":"df6fa350-5435-4bc9-8412-6c60106e1e20"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"NpOvN6MDRq0j","executionInfo":{"status":"ok","timestamp":1680682240009,"user_tz":-60,"elapsed":4086,"user":{"displayName":"Haohan Zhu","userId":"13394524445423274234"}}},"outputs":[],"source":["import os\n","import xml.etree.ElementTree as ET\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import torch\n","from os.path import join\n","from torchvision.io import read_image\n","import numpy as np\n","import cv2\n","from torchvision import transforms\n","import torch.nn as nn\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"JdP1gJ-qYgsk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680609336908,"user_tz":-60,"elapsed":13,"user":{"displayName":"Haohan Zhu","userId":"13394524445423274234"}},"outputId":"7f5c9f6e-9e1f-4153-a09d-ea2d0de5ed41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Apr  4 11:55:35 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P0    43W / 400W |      0MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["torch.cuda.is_available()"],"metadata":{"id":"BYEggqp9YyBv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680609336908,"user_tz":-60,"elapsed":8,"user":{"displayName":"Haohan Zhu","userId":"13394524445423274234"}},"outputId":"f12ce119-9635-4b18-d975-4aa40d077e28"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["class BlinkingDataset(Dataset):\n","\n","    def __init__(self, dataset, split_path='train', clip_len = 3, transform=None, target_transform=None):\n","        self.dataset = dataset\n","        self.root_dir = '/content/gdrive/MyDrive/dataset'\n","        if not self.check_integrity():\n","            raise RuntimeError('Dataset not found or corrupted.' +\n","                               ' Please check the dataset location.')\n","        \n","        self.split_path = split_path\n","        self.split_dir = os.path.join(self.root_dir, self.split_path)\n","        self.clip_len = clip_len\n","\n","        self.target_transform = target_transform\n","        self.transform = transform\n","\n","        \n","\n","        self.class_names = ['Fixed', 'Blinking']\n","        self.class_dict = {class_name: i for i, class_name in enumerate(self.class_names)}\n","\n","        image_dir = 'images'\n","        annotation_dir = 'Annotations'\n","        \n","        # if preprocessing:\n","        #     print('Preprocessing of {} dataset, this will take long, but it will be done only once.'.format(dataset))\n","            # self.preprocess()\n","\n","        self.img_batch_path = []\n","        self.ann_batch_path = []\n","        img_list = []\n","        for sub_dir in os.listdir(self.split_dir): \n","            sub_path = join(self.split_dir, sub_dir)\n","\n","            annotation_path = join(sub_path, annotation_dir)\n","            image_path = join(sub_path, image_dir)\n","\n","            imgfiles = [f for f in os.listdir(image_path) if os.path.isfile(join(image_path, f))]\n","            annfiles = [f for f in os.listdir(annotation_path) if os.path.isfile(join(annotation_path, f))]\n","            \n","            \n","            for i in range(0, len(imgfiles) - self.clip_len + 1):\n","                batch = imgfiles[i : i+self.clip_len]\n","                batch_path = [os.path.join(image_path, file_name) for file_name in batch if os.path.isfile(os.path.join(image_path, file_name))]\n","                self.img_batch_path.append(batch_path)\n","            \n","                \n","            for i in range(0, len(annfiles) - self.clip_len + 1, 1):\n","                batch = annfiles[i : i+self.clip_len]\n","                batch_path = [os.path.join(annotation_path, file_name) for file_name in batch if os.path.isfile(os.path.join(annotation_path, file_name))]\n","                self.ann_batch_path.append(batch_path)    \n","\n","    def __len__(self):\n","        return len(self.img_batch_path)\n","        \n","    def __getitem__(self, index):\n","        img_paths = self.img_batch_path[index]\n","        ann_paths = self.ann_batch_path[index]\n","        \n","        images = torch.Tensor()\n","        bboxes = []\n","        labels = []\n","        imgs_list = []\n","        for x in range(len(img_paths)):\n","            img = Image.open(img_paths[x]).convert('RGB')\n","            # img = img.resize((224,224))\n","            img = transforms.ToTensor()(img)\n","\n","            imgs_list.append(img)\n","\n","        images = torch.stack(imgs_list, dim = 1).to(device)\n","\n","        for f in range(len(ann_paths)):\n","            label_per_img = []\n","            bbox_per_img = []\n","            tree = ET.parse(ann_paths[f])\n","            root = tree.getroot()\n","            size = root.find('size')\n","            width = float(size.find('width').text)\n","            height = float(size.find('height').text)\n","            depth = float(size.find('depth').text)\n","\n","            for obj in root.findall('object'):\n","                name = obj.find('name').text\n","                if name not in self.class_names:\n","                    continue\n","                for bounding_box in obj.iter('bndbox'):\n","                    xmin = float(bounding_box.find('xmin').text)\n","                    ymin = float(bounding_box.find('ymin').text)\n","                    xmax = float(bounding_box.find('xmax').text)\n","                    ymax = float(bounding_box.find('ymax').text)\n","                    \n","                    # xmin = xmin / width\n","                    # ymin = ymin / height\n","                    # xmax = xmax / width\n","                    # ymax = ymax / height\n","                    box = [xmin, ymin, xmax, ymax]\n","\n","                    bbox_per_img.append(box)\n","                    label_per_img.append(self.class_dict[name])\n","\n","            bboxes.append(torch.Tensor(bbox_per_img))\n","            labels.append(torch.LongTensor(label_per_img))\n","            \n","        return {\n","            'images': images,\n","            'bboxes': bboxes,\n","            'labels': labels\n","        }\n","    \n","    def check_integrity(self):\n","        if not os.path.exists(self.root_dir):\n","            return False\n","        else:\n","            return True"],"metadata":{"id":"rwdchT-e2w69","executionInfo":{"status":"ok","timestamp":1680682240009,"user_tz":-60,"elapsed":12,"user":{"displayName":"Haohan Zhu","userId":"13394524445423274234"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torch.nn.functional as F\n","\n","\n","def generate_anchors(scales, aspect_ratios):\n","    anchors = []\n","    for scale in scales:\n","        for aspect_ratio in aspect_ratios:\n","            h = scale / torch.sqrt(torch.tensor(aspect_ratio))\n","            w = scale * torch.sqrt(torch.tensor(aspect_ratio))\n","            anchors.append([-w / 2, -h / 2, w / 2, h / 2])\n","    return torch.tensor(anchors, dtype=torch.float32)\n","\n","\n","\n","class FPN(nn.Module):\n","    def __init__(self, C3_size, C4_size, C5_size, feature_size=256):\n","        super(FPN, self).__init__()\n","\n","        self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)\n","        self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n","\n","        self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)\n","        self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n","\n","        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)\n","        self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n","\n","    def forward(self, x):\n","        C3, C4, C5 = x\n","\n","        P5 = self.P5_1(C5)\n","        P5_upsampled = F.interpolate(P5, scale_factor=2, mode='bilinear')\n","        \n","        # Match dimensions by cropping or padding\n","        diff_h = P5_upsampled.size(2) - C4.size(2)\n","        diff_w = P5_upsampled.size(3) - C4.size(3)\n","\n","        if diff_h > 0:\n","            P5_upsampled = P5_upsampled[:, :, :-diff_h, :]\n","        elif diff_h < 0:\n","            P5_upsampled = F.pad(P5_upsampled, (0, 0, 0, -diff_h))\n","\n","        if diff_w > 0:\n","            P5_upsampled = P5_upsampled[:, :, :, :-diff_w]\n","        elif diff_w < 0:\n","            P5_upsampled = F.pad(P5_upsampled, (0, -diff_w, 0, 0))\n","\n","        P4 = self.P4_1(C4) + P5_upsampled\n","        \n","        P4_upsampled = F.interpolate(P4, scale_factor=2)\n","        if P4_upsampled.shape[-1] > self.P3_1(C3).shape[-1]:\n","            P4_upsampled = P4_upsampled[..., :-1]\n","        P3 = self.P3_1(C3) + P4_upsampled\n","\n","        P3 = self.P3_2(P3)\n","        P4 = self.P4_2(P4)\n","        P5 = self.P5_2(P5)\n","\n","        return [P3, P4, P5]\n"],"metadata":{"id":"2ayQ7M2MSCsR","executionInfo":{"status":"ok","timestamp":1680682240009,"user_tz":-60,"elapsed":11,"user":{"displayName":"Haohan Zhu","userId":"13394524445423274234"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","from torchvision.ops import box_iou\n","\n","\n","class RetinaNet(nn.Module):\n","    def __init__(self, num_classes, scales, aspect_ratios, attention_size=256, lstm_hidden_size=512, lstm_num_layers=2):\n","        super(RetinaNet, self).__init__()\n","        \n","        # Load a pretrained resnet50 model\n","        resnet = models.resnet50(pretrained=True)\n","        \n","        self.C3 = nn.Sequential(*list(resnet.children())[:6])\n","        self.C4 = list(resnet.children())[6]\n","        self.C5 = list(resnet.children())[7]\n","\n","        # Create the FPN module\n","        self.fpn = FPN(512, 1024, 2048)\n","\n","        # LSTM and Attention Mechanism\n","        self.lstm = nn.LSTM(256 * 3, lstm_hidden_size, lstm_num_layers, batch_first=True)\n","        self.attention = nn.MultiheadAttention(lstm_hidden_size, num_heads=8)\n","        \n","        # Classification and Regression heads\n","        num_anchors = len(scales) * len(aspect_ratios)\n","        self.classification_head = nn.Linear(lstm_hidden_size, num_classes * num_anchors)\n","        self.regression_head = nn.Linear(lstm_hidden_size, 4 * num_anchors)\n","\n","        # Anchor generation\n","        self.anchors = generate_anchors(scales, aspect_ratios)\n","\n","    def forward(self, x):\n","        B, T, C, H, W = x.size()\n","        x = x.view(B * T, C, H, W)\n","        \n","        C3 = self.C3(x)\n","        C4 = self.C4(C3)\n","        C5 = self.C5(C4)\n","        \n","        features = self.fpn([C3, C4, C5])\n","\n","        # Merge the FPN feature maps along the channel axis\n","        merged_features = torch.cat(features, dim=1)\n","\n","        # Reshape back to (B, T, C, H, W)\n","        merged_features = merged_features.view(B, T, -1, merged_features.shape[2], merged_features.shape[3])\n","\n","        # Global Average Pooling\n","        gap = nn.AdaptiveAvgPool2d(1)(merged_features)\n","        gap = gap.view(B, T, -1)\n","\n","        # LSTM\n","        lstm_out, _ = self.lstm(gap)\n","\n","        # Attention Mechanism\n","        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n","\n","        # Classification and Regression heads\n","        classification = self.classification_head(attn_out[:, -1, :]).view(B, -1, num_classes)\n","        regression = self.regression_head(attn_out[:, -1, :]).view(B, -1, 4)\n","\n","        return classification, regression\n"],"metadata":{"id":"yeN9m-8AYnSL","executionInfo":{"status":"ok","timestamp":1680682240010,"user_tz":-60,"elapsed":12,"user":{"displayName":"Haohan Zhu","userId":"13394524445423274234"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","from torch.utils.data import DataLoader\n","torch.cuda.empty_cache()\n","\n","# Loss functions\n","def focal_loss(classification, targets, alpha=0.25, gamma=2):\n","    classification_loss = torch.nn.CrossEntropyLoss(reduction=\"none\")(classification, targets)\n","    p_t = torch.exp(-classification_loss)\n","    focal_loss = alpha * (1 - p_t)**gamma * classification_loss\n","    return focal_loss.mean()\n","\n","def smooth_l1_loss(regression, targets, sigma=1.0):\n","    regression_diff = regression - targets\n","    regression_loss = torch.where(torch.abs(regression_diff) < 1 / sigma**2,\n","                                  0.5 * sigma**2 * regression_diff**2,\n","                                  torch.abs(regression_diff) - 0.5 / sigma**2)\n","    return regression_loss.mean()\n","\n","# Instantiate the RetinaNet model\n","num_classes = 2\n","scales = [32, 64, 128]\n","aspect_ratios = [0.5, 1, 2]\n","\n","model = RetinaNet(num_classes, scales, aspect_ratios)\n","\n","# Create the DataLoader\n","train_dataset = BlinkingDataset(dataset = 'FRSign', split_path='train')\n","train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n","\n","\n","# Loss functions\n","classification_criterion = nn.CrossEntropyLoss()\n","regression_criterion = nn.SmoothL1Loss()\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# # Optimizer\n","# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","num_epochs = 50\n","\n","model.to(device)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TjJKBzQ1Tn3f","executionInfo":{"status":"ok","timestamp":1680682252851,"user_tz":-60,"elapsed":12853,"user":{"displayName":"Haohan Zhu","userId":"13394524445423274234"}},"outputId":"899f4d67-853e-4f20-eaf9-b3c6093aee6d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 206MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["RetinaNet(\n","  (C3): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (4): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (5): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","  )\n","  (C4): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (4): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (5): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (C5): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (fpn): FPN(\n","    (P5_1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (P5_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (P4_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (P4_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (P3_1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (P3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  )\n","  (lstm): LSTM(768, 512, num_layers=2, batch_first=True)\n","  (attention): MultiheadAttention(\n","    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","  )\n","  (classification_head): Linear(in_features=512, out_features=18, bias=True)\n","  (regression_head): Linear(in_features=512, out_features=36, bias=True)\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["for epoch in range(num_epochs):\n","    model.train()\n","\n","    running_loss = 0.0\n","    for batch_idx, batch in enumerate(train_loader):\n","        images = batch['images']\n","        gt_bboxes = batch['bboxes']\n","        gt_labels = batch['labels']\n","\n","        # Forward pass\n","        pred_classification, pred_regression = model(images)\n","\n","        # Calculate loss\n","        classification_loss, regression_loss = 0, 0\n","        for b in range(images.size(0)):\n","            # Calculate classification loss\n","            classification_loss += classification_criterion(pred_classification[b], gt_labels[b])\n","\n","            # Calculate regression loss\n","            regression_loss += regression_criterion(pred_regression[b], gt_bboxes[b])\n","\n","        loss = classification_loss + regression_loss\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    # Print epoch results\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / (batch_idx + 1)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":401},"id":"nFQB2x4LYjze","executionInfo":{"status":"error","timestamp":1680682259112,"user_tz":-60,"elapsed":6272,"user":{"displayName":"Haohan Zhu","userId":"13394524445423274234"}},"outputId":"9c34bfbb-eeb3-4eb7-e2b6-bbbe2716180e"},"execution_count":7,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-516425d92a52>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mpred_classification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_regression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-6c5f7f08b9a3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Merge the FPN feature maps along the channel axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mmerged_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Reshape back to (B, T, C, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 150 but got size 75 for tensor number 1 in the list."]}]}]}